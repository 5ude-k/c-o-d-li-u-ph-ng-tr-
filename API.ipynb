{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f8a44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Trang 1 | Offset 0 ‚Üí 50 tin (Total_ads=None)\n",
      "üìÑ Trang 2 | Offset 50 ‚Üí 50 tin (Total_ads=None)\n",
      "üìÑ Trang 3 | Offset 100 ‚Üí 44 tin (Total_ads=None)\n",
      "üìÑ Trang 4 | Offset 150 ‚Üí 0 tin (Total_ads=None)\n",
      "Kh√¥ng c√≤n d·ªØ li·ªáu ‚Üí D·ª´ng.\n",
      "\n",
      "‚úÖ Ho√†n t·∫•t! ƒê√£ l∆∞u 144 tin v√†o nhatot_phongtro_danang_all.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "LISTING_URL = \"https://gateway.chotot.com/v1/public/ad-listing\"\n",
    "DETAIL_URL = \"https://gateway.chotot.com/v1/public/ad-listing/{}\"\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Origin\": \"https://www.nhatot.com\",\n",
    "    \"Referer\": \"https://www.nhatot.com/\",\n",
    "    \"cookie\": \"__cf_bm=PASTE_YOUR_COOKIE_HERE\"\n",
    "}\n",
    "\n",
    "# Thu√™ ph√≤ng tr·ªç t·∫°i ƒê√† N·∫µng\n",
    "PARAMS_BASE = {\n",
    "    \"region_v2\": 3017,  # ƒê√† N·∫µng\n",
    "    \"cg\": 1050,         # Cho thu√™ ph√≤ng tr·ªç\n",
    "    \"st\": \"u,h\",        # tr·∫°ng th√°i tin\n",
    "    \"limit\": 50,        # s·ªë tin m·ªói trang\n",
    "}\n",
    "\n",
    "# ------------------ H√ÄM PH·ª§ TR·ª¢ ------------------\n",
    "def detect_features(text):\n",
    "    \"\"\"Tr√≠ch xu·∫•t c√°c ti·ªán √≠ch t·ª´ m√¥ t·∫£\"\"\"\n",
    "    if not text:\n",
    "        return {\"M√°y gi·∫∑t\": 0, \"T·ªß l·∫°nh\": 0, \"Wifi\": 0, \"ƒêi·ªÅu h√≤a\": 0}\n",
    "    t = text.lower()\n",
    "    return {\n",
    "        \"M√°y gi·∫∑t\": 1 if re.search(r\"m√°y[\\s\\-]?gi·∫∑t|may giat|gi·∫∑t\", t) else 0,\n",
    "        \"T·ªß l·∫°nh\": 1 if re.search(r\"t·ªß[\\s\\-]?l·∫°nh|tu lanh|tulanh\", t) else 0,\n",
    "        \"Wifi\": 1 if re.search(r\"wi[-\\s]?fi|wifi\", t) else 0,\n",
    "        \"ƒêi·ªÅu h√≤a\": 1 if re.search(r\"ƒëi·ªÅu[\\s\\-]?h√≤a|dieu hoa|m√°y[\\s\\-]?l·∫°nh|ƒëh\\b\", t) else 0,\n",
    "    }\n",
    "\n",
    "def safe_get_detail(ad_id, headers, max_retries=3):\n",
    "    \"\"\"L·∫•y chi ti·∫øt tin ƒëƒÉng (m√¥ t·∫£ + tham s·ªë)\"\"\"\n",
    "    backoff = 0.5\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            r = requests.get(DETAIL_URL.format(ad_id), headers=headers, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                d = r.json()\n",
    "                params_dict = {p.get(\"id\"): p.get(\"value\") for p in d.get(\"params\", []) if isinstance(p, dict)}\n",
    "                desc = d.get(\"body\", \"\") or \"\"\n",
    "                return params_dict, desc\n",
    "            elif r.status_code in (403, 429):\n",
    "                time.sleep(backoff * 4)\n",
    "            else:\n",
    "                time.sleep(backoff)\n",
    "        except Exception:\n",
    "            time.sleep(backoff)\n",
    "        backoff *= 1.8\n",
    "    return {}, \"\"\n",
    "\n",
    "# ------------------ C√ÄO D·ªÆ LI·ªÜU ------------------\n",
    "all_data = []\n",
    "offset = 0\n",
    "limit = PARAMS_BASE[\"limit\"]\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    params = {**PARAMS_BASE, \"o\": offset}\n",
    "    try:\n",
    "        r = requests.get(LISTING_URL, params=params, headers=HEADERS, timeout=15)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è L·ªói {r.status_code} t·∫°i offset={offset}\")\n",
    "            break\n",
    "        data = r.json()\n",
    "        ads = data.get(\"ads\", [])\n",
    "        total_ads = data.get(\"total_ads\")\n",
    "        print(f\"üìÑ Trang {page} | Offset {offset} ‚Üí {len(ads)} tin (Total_ads={total_ads})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è L·ªói k·∫øt n·ªëi: {e}\")\n",
    "        break\n",
    "\n",
    "    if not ads:\n",
    "        print(\"Kh√¥ng c√≤n d·ªØ li·ªáu ‚Üí D·ª´ng.\")\n",
    "        break\n",
    "\n",
    "    for ad in ads:\n",
    "        ad_id = ad.get(\"list_id\")\n",
    "        if not ad_id:\n",
    "            continue\n",
    "\n",
    "        price_str = ad.get(\"price_string\", \"\")\n",
    "        if not price_str:\n",
    "            continue\n",
    "\n",
    "        # --- L·∫•y chi ti·∫øt ---\n",
    "        params_dict, desc = safe_get_detail(ad_id, HEADERS)\n",
    "        if not desc:\n",
    "            desc = ad.get(\"body\", \"\") or \"\"\n",
    "\n",
    "        features = detect_features(desc)\n",
    "\n",
    "        all_data.append({\n",
    "            \"Ti√™u ƒë·ªÅ\": ad.get(\"subject\"),\n",
    "            \"Gi√°\": price_str,\n",
    "            \"Di·ªán t√≠ch\": f\"{ad.get('size')} m¬≤\" if ad.get(\"size\") else None,\n",
    "            \"ƒê·ªãa ch·ªâ\": f\"{ad.get('area_name', '')}, {ad.get('region_name', '')}\".strip(\", \"),\n",
    "            \"N·ªôi th·∫•t\": params_dict.get(\"furnishing_rent\", params_dict.get(\"furnishing\", \"0\")),\n",
    "            **features,\n",
    "            \"Link\": f\"https://www.nhatot.com/cho-thue-phong-tro/{ad_id}.htm\"\n",
    "        })\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    offset += limit\n",
    "    page += 1\n",
    "\n",
    "    if total_ads and offset >= total_ads:\n",
    "        print(\"üéØ ƒê√£ l·∫•y ƒë·ªß t·ªïng s·ªë tin API b√°o ‚Üí D·ª´ng.\")\n",
    "        break\n",
    "\n",
    "    time.sleep(0.8)\n",
    "\n",
    "# ------------------ L∆ØU FILE ------------------\n",
    "df = pd.DataFrame(all_data)\n",
    "out_file = \"nhatot_phongtro_danang_all.xlsx\"\n",
    "df.to_excel(out_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ho√†n t·∫•t! ƒê√£ l∆∞u {len(df)} tin v√†o {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f97f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to SQLite: nhatot.db\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.types import Integer, Float, String, Boolean\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "EXCEL_FILE = \"nhatot_phongtro_danang_all.xlsx\"   # file xu·∫•t t·ª´ crawler\n",
    "# SQLite (local)\n",
    "SQLITE_DB = \"nhatot.db\"\n",
    "# PostgreSQL example (replace user:pass@host:port/dbname)\n",
    "PG_CONN = \"postgresql+psycopg2://user:password@localhost:5432/nhatot_db\"\n",
    "# MySQL example (replace user:pass@host:port/dbname)\n",
    "MYSQL_CONN = \"mysql+pymysql://user:password@localhost:3306/nhatot_db\"\n",
    "\n",
    "TABLE_NAME = \"nhatot_phongtro\"\n",
    "# choose which DB to use: \"sqlite\", \"postgres\", \"mysql\"\n",
    "DB_TYPE = \"sqlite\"\n",
    "# ----------------------------------------\n",
    "\n",
    "def extract_ad_id_from_link(link):\n",
    "    if not isinstance(link, str):\n",
    "        return None\n",
    "    m = re.search(r\"/(\\d+)(?:\\.htm|$)\", link)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def prepare_dataframe(path):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
    "    df = pd.read_excel(path)\n",
    "    # ensure ad_id column present\n",
    "    if \"ad_id\" not in df.columns and \"ID\" not in df.columns:\n",
    "        # try to extract from Link column\n",
    "        if \"Link\" in df.columns:\n",
    "            df[\"ad_id\"] = df[\"Link\"].apply(extract_ad_id_from_link)\n",
    "        else:\n",
    "            df[\"ad_id\"] = None\n",
    "    else:\n",
    "        if \"ID\" in df.columns and \"ad_id\" not in df.columns:\n",
    "            df = df.rename(columns={\"ID\": \"ad_id\"})\n",
    "    # normalize column names: remove leading/trailing spaces\n",
    "    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]\n",
    "    # optional: normalize price -> numeric (simple)\n",
    "    if \"Gi√°\" in df.columns:\n",
    "        def parse_price(s):\n",
    "            try:\n",
    "                if pd.isna(s): return None\n",
    "                s = str(s).lower().replace(\",\", \"\")\n",
    "                m_tr = re.search(r\"([\\d\\.]+)\\s*tri·ªá\", s)\n",
    "                m_ty = re.search(r\"([\\d\\.]+)\\s*t·ª∑\", s)\n",
    "                if m_tr:\n",
    "                    return float(m_tr.group(1))\n",
    "                if m_ty:\n",
    "                    return float(m_ty.group(1)) * 1000.0\n",
    "                # digits only fallback\n",
    "                nums = re.findall(r\"(\\d{3,})\", re.sub(r\"\\D\",\"\", s))\n",
    "                if nums:\n",
    "                    return int(nums[0]) / 1_000_000.0\n",
    "            except:\n",
    "                return None\n",
    "            return None\n",
    "        df[\"price_million_vnd\"] = df[\"Gi√°\"].apply(parse_price)\n",
    "    return df\n",
    "\n",
    "# ---------- SQLite simple append ----------\n",
    "def push_sqlite(df, sqlite_path=SQLITE_DB, table=TABLE_NAME):\n",
    "    engine = create_engine(f\"sqlite:///{sqlite_path}\", future=True)\n",
    "    # create index/unique on ad_id if not exists (sqlite: create table if not exists then create unique index)\n",
    "    with engine.begin() as conn:\n",
    "        # create table via pandas if not exists (append will create)\n",
    "        df.to_sql(table, conn, if_exists=\"append\", index=False)\n",
    "        # create unique index on ad_id to avoid duplicates (may fail if duplicates exist)\n",
    "        try:\n",
    "            conn.execute(text(f\"CREATE UNIQUE INDEX IF NOT EXISTS ux_{table}_adid ON {table}(ad_id);\"))\n",
    "        except Exception as e:\n",
    "            print(\"Warning creating unique index:\", e)\n",
    "    engine.dispose()\n",
    "    print(\"Saved to SQLite:\", sqlite_path)\n",
    "\n",
    "# ---------- PostgreSQL with upsert (recommended) ----------\n",
    "def push_postgres_upsert(df, pg_conn=PG_CONN, table=TABLE_NAME, chunk_size=500):\n",
    "    engine = create_engine(pg_conn, future=True)\n",
    "    # Ensure dataframe has ad_id (required for conflict target)\n",
    "    if \"ad_id\" not in df.columns:\n",
    "        raise ValueError(\"ad_id column required for upsert to Postgres\")\n",
    "    # Create temp table and bulk insert then upsert\n",
    "    tmp_table = f\"{table}_tmp\"\n",
    "    with engine.begin() as conn:\n",
    "        # 1) create target table if not exists by writing empty df (only first time)\n",
    "        # We'll create with pandas if not exists\n",
    "        existing = conn.execute(text(\n",
    "            \"SELECT to_regclass(:tname)\"),\n",
    "            {\"tname\": table}).scalar()\n",
    "        if not existing:\n",
    "            # create by sending empty df.head(0)\n",
    "            df.head(0).to_sql(table, conn, if_exists=\"fail\", index=False)\n",
    "            print(f\"Created table {table}\")\n",
    "        # 2) write to temp table\n",
    "        # drop temp if exists\n",
    "        conn.execute(text(f\"DROP TABLE IF EXISTS {tmp_table};\"))\n",
    "        df.to_sql(tmp_table, conn, if_exists=\"replace\", index=False)\n",
    "        # 3) upsert from temp -> target using ad_id as unique key\n",
    "        # you may want to adjust columns list to your real columns\n",
    "        cols = list(df.columns)\n",
    "        cols_quoted = \", \".join([f'\"{c}\"' for c in cols])\n",
    "        cols_assign = \", \".join([f'{c}=EXCLUDED.\"{c}\"' for c in cols if c != \"ad_id\"])\n",
    "        sql = f'''\n",
    "        INSERT INTO \"{table}\" ({cols_quoted})\n",
    "        SELECT {cols_quoted} FROM \"{tmp_table}\"\n",
    "        ON CONFLICT (ad_id) DO UPDATE SET {cols_assign};\n",
    "        '''\n",
    "        conn.execute(text(sql))\n",
    "        # drop temp\n",
    "        conn.execute(text(f\"DROP TABLE IF EXISTS {tmp_table};\"))\n",
    "    engine.dispose()\n",
    "    print(\"Upserted into Postgres table:\", table)\n",
    "\n",
    "# ---------- MySQL simple upsert example ----------\n",
    "def push_mysql_upsert(df, mysql_conn=MYSQL_CONN, table=TABLE_NAME):\n",
    "    engine = create_engine(mysql_conn, future=True)\n",
    "    # ensure ad_id exists\n",
    "    if \"ad_id\" not in df.columns:\n",
    "        raise ValueError(\"ad_id column required for upsert to MySQL\")\n",
    "    with engine.begin() as conn:\n",
    "        # create table if not exists\n",
    "        df.head(0).to_sql(table, conn, if_exists=\"append\", index=False)\n",
    "        # MySQL upsert: iterate rows (for simplicity) ‚Äî for large data, use LOAD DATA INFILE or other bulk methods\n",
    "        cols = list(df.columns)\n",
    "        col_list = \",\".join([f\"`{c}`\" for c in cols])\n",
    "        placeholders = \",\".join([f\":{c}\" for c in cols])\n",
    "        update_list = \",\".join([f\"`{c}`=VALUES(`{c}`)\" for c in cols if c != \"ad_id\"])\n",
    "        insert_sql = f\"INSERT INTO `{table}` ({col_list}) VALUES ({placeholders}) ON DUPLICATE KEY UPDATE {update_list}\"\n",
    "        # ensure ad_id has UNIQUE index (user should create it beforehand)\n",
    "        for row in df.to_dict(orient=\"records\"):\n",
    "            conn.execute(text(insert_sql), row)\n",
    "    engine.dispose()\n",
    "    print(\"Upserted into MySQL table:\", table)\n",
    "\n",
    "# ---------------- Example usage ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = prepare_dataframe(EXCEL_FILE)\n",
    "    # choose engine:\n",
    "    if DB_TYPE == \"sqlite\":\n",
    "        push_sqlite(df)\n",
    "    elif DB_TYPE == \"postgres\":\n",
    "        push_postgres_upsert(df)\n",
    "    elif DB_TYPE == \"mysql\":\n",
    "        push_mysql_upsert(df)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown DB_TYPE\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
